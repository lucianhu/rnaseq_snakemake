Building DAG of jobs...
shared_storage_local_copies: True
remote_exec: False
Using shell: /usr/bin/bash
Provided cores: 24
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                            count
---------------------------  -------
all                                1
picard_mark_duplicates             6
samtools_markdup_processing        6
total                             13

Resources before job selection: {'_cores': 24, '_nodes': 9223372036854775807}
Ready jobs (6)
Select jobs to execute...
Selected jobs (4)
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775803}
Execute 4 jobs...

[Thu Aug 15 16:50:04 2024]
Job 14: WT_REP2_control: Marking duplicate reads
Reason: Missing output files: results/align/bam_markup/WT_REP2_control/WT_REP2_control.markdup.sorted.bam


[Thu Aug 15 16:50:04 2024]
Job 17: RAP1_REP2_treatment: Marking duplicate reads
Reason: Missing output files: results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam


[Thu Aug 15 16:50:04 2024]
Job 16: RAP1_REP1_treatment: Marking duplicate reads
Reason: Missing output files: results/align/bam_markup/RAP1_REP1_treatment/RAP1_REP1_treatment.markdup.sorted.bam


[Thu Aug 15 16:50:04 2024]
Job 13: WT_REP1_control: Marking duplicate reads
Reason: Missing output files: results/align/bam_markup/WT_REP1_control/WT_REP1_control.markdup.sorted.bam

Full Traceback (most recent call last):
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/executors/local.py", line 423, in run_wrapper
    run(
  File "/home/lucianhu/rna_snakemake/test_ting/Snakefile", line 604, in __rule_picard_mark_duplicates
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/shell.py", line 357, in __new__
    raise sp.CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command 'set -euo pipefail;  
        picard -Xmx29491M MarkDuplicates         --ASSUME_SORTED true         --REMOVE_DUPLICATES false         --VALIDATION_STRINGENCY LENIENT         --TMP_DIR tmp         --INPUT results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam         --OUTPUT results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam         --REFERENCE_SEQUENCE resources/genome.fa         --METRICS_FILE results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt         &> logs/markdup/RAP1_REP2_treatment.markduplicates.log' returned non-zero exit status 137.

Full Traceback (most recent call last):
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/executors/local.py", line 423, in run_wrapper
    run(
  File "/home/lucianhu/rna_snakemake/test_ting/Snakefile", line 604, in __rule_picard_mark_duplicates
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/shell.py", line 357, in __new__
    raise sp.CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command 'set -euo pipefail;  
        picard -Xmx29491M MarkDuplicates         --ASSUME_SORTED true         --REMOVE_DUPLICATES false         --VALIDATION_STRINGENCY LENIENT         --TMP_DIR tmp         --INPUT results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam         --OUTPUT results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam         --REFERENCE_SEQUENCE resources/genome.fa         --METRICS_FILE results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt         &> logs/markdup/RAP1_REP2_treatment.markduplicates.log' returned non-zero exit status 137.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/executors/local.py", line 261, in _callback
    raise ex
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/executors/local.py", line 247, in cached_or_run
    run_func(*args)
  File "/home/lucianhu/micromamba/envs/rna_snakemake/lib/python3.12/site-packages/snakemake/executors/local.py", line 459, in run_wrapper
    raise RuleException(
snakemake.exceptions.RuleException: CalledProcessError in file /home/lucianhu/rna_snakemake/test_ting/Snakefile, line 312:
Command 'set -euo pipefail;  
        picard -Xmx29491M MarkDuplicates         --ASSUME_SORTED true         --REMOVE_DUPLICATES false         --VALIDATION_STRINGENCY LENIENT         --TMP_DIR tmp         --INPUT results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam         --OUTPUT results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam         --REFERENCE_SEQUENCE resources/genome.fa         --METRICS_FILE results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt         &> logs/markdup/RAP1_REP2_treatment.markduplicates.log' returned non-zero exit status 137.

RuleException:
CalledProcessError in file /home/lucianhu/rna_snakemake/test_ting/Snakefile, line 312:
Command 'set -euo pipefail;  
        picard -Xmx29491M MarkDuplicates         --ASSUME_SORTED true         --REMOVE_DUPLICATES false         --VALIDATION_STRINGENCY LENIENT         --TMP_DIR tmp         --INPUT results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam         --OUTPUT results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam         --REFERENCE_SEQUENCE resources/genome.fa         --METRICS_FILE results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt         &> logs/markdup/RAP1_REP2_treatment.markduplicates.log' returned non-zero exit status 137.
[Thu Aug 15 16:50:14 2024]
Error in rule picard_mark_duplicates:
    jobid: 17
    input: results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam, resources/genome.fa
    output: results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam, results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt
    log: logs/markdup/RAP1_REP2_treatment.markduplicates.log (check log file(s) for error details)
    conda-env: /home/lucianhu/rna_snakemake/test_ting/.snakemake/conda/fcc45f35c3a7a558a9856280ef901482_
    shell:
        
        picard -Xmx29491M MarkDuplicates         --ASSUME_SORTED true         --REMOVE_DUPLICATES false         --VALIDATION_STRINGENCY LENIENT         --TMP_DIR tmp         --INPUT results/align/bam_original/RAP1_REP2_treatment/RAP1_REP2_treatment.sorted.bam         --OUTPUT results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.bam         --REFERENCE_SEQUENCE resources/genome.fa         --METRICS_FILE results/align/bam_markup/RAP1_REP2_treatment/RAP1_REP2_treatment.markdup.sorted.MarkDuplicates.metrics.txt         &> logs/markdup/RAP1_REP2_treatment.markduplicates.log
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

